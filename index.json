[{"content":" This is a course assignment demo (GG606 Scientific Data Wrangling)\n  cancensus is a R package that can assess Statistics Canada Census data for Census year 1996, 2001, 2006, 2011, 2016 and 2021. The datasets present information from the Census of Population for various levels of geography, including provinces and territories, census metropolitan areas, communities and census tracts.\n 1. Installation and retrieve the data vectors list install.packages(\u0026#34;cancensus\u0026#34;) library(cancensus) #view avaliable Census data list_census_datasets() #view regions and vectors in a given dataset list_census_regions(\u0026#34;CA21\u0026#34;) list_census_vectors(\u0026#34;CA21\u0026#34;) #retrieve the data needed #Warning: Cached regions list may be out of date. Set `use_cache = FALSE` to update it. library(geojsonsf) dataset \u0026lt;- \u0026#34;CA21\u0026#34; #set census api #set_cancensus_api_key(\u0026#39;ENTER API KEY\u0026#39;) #set_cancensus_cache_path(here(\u0026#34;data\u0026#34;)) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\r## ✔ ggplot2 3.4.0 ✔ purrr 1.0.0 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10\r## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\r## ✖ dplyr::filter() masks stats::filter()\r## ✖ dplyr::lag() masks stats::lag()\r regions.list \u0026lt;- list_census_regions(dataset, use_cache = FALSE) %\u0026gt;% filter(level == \u0026#34;PR\u0026#34;, name == \u0026#34;Alberta\u0026#34;) %\u0026gt;% as_census_region_list vec \u0026lt;- find_census_vectors(\u0026#34;housing\u0026#34;, dataset = dataset, query_type = \u0026#34;semantic\u0026#34;) 2. Housing situation in Alberta 2.1 Dwelling Value # getting 2021 data # selecting vectors in need # Median Dwelling value dwelling.21.cost \u0026lt;- get_census(dataset = \u0026#34;CA21\u0026#34;, regions = regions.list, vectors = c(\u0026#34;median.dwelling\u0026#34;=\u0026#34;v_CA21_4311\u0026#34;), level = \u0026#34;CSD\u0026#34;, labels = \u0026#39;short\u0026#39;, geo_format = \u0026#39;sf\u0026#39;) dwelling.21.value \u0026lt;- get_census(dataset = \u0026#34;CA21\u0026#34;, regions = regions.list, vectors = c(\u0026#34;median.dwelling\u0026#34;=\u0026#34;v_CA21_4311\u0026#34;), level = \u0026#34;CSD\u0026#34;, labels = \u0026#39;short\u0026#39;) %\u0026gt;% slice_max(median.dwelling, prop = .05) ggplot(dwelling.21.cost) + geom_sf(aes(fill=median.dwelling)) + theme_minimal() + theme(panel.grid = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), strip.text.x = element_text(size=12)) + coord_sf(datum=NA) + scale_fill_viridis_c(\u0026#34;Median Dwellings Value\u0026#34;, labels = scales::dollar) By looking at general housing situation, I first started with the price of dwellings. The first figure showed the distribution of median dwelling value in Alberta. In a large number of district, the value of dwellings are around $500,000 and above. Roughly, the price of dwellings is more expensive in the southern Alberta compared with the North, which might related to what kind of climate people want to live in. There is a special pattern around the district of Calgary, where in the center of Calgary, the price of dwellings is far less than its surrounding area. The similar, but less obvious pattern could be found around Edmonton. The reason may relate to people prefer to live in suburban area compared with living in downtown, but the gap of dwellings value around Calgary is astonishing. Besides, the surrounding cycle outside Alberta is the most expensive area in terms of dwellings value in the province. The next figure will look into it in more detail.\noptions(scipen = 999) ggplot(dwelling.21.value, aes(x=median.dwelling, y=fct_reorder(`Region Name`, median.dwelling))) + geom_point() + theme_minimal() + theme(axis.title.y = element_blank()) + xlab(\u0026#34;Median Dwellings Value\u0026#34;) + ggtitle(\u0026#34;Districts with Top 5% Median Dwellings Value in Alberta\u0026#34;) The second figure looked into dwelling value in more detail: showing the districts with top 5% median dwelling value. 25 out of 423 districts in Alberta have been selected with higher median dwelling value, and particularly, all district with median dwelling value higher than $500,000 have been selected. Apparently, the district around Calgary have all been included, and also the districts to the west of Calgary. By Google-search these district names, I realized these districts located in a good place for relax and vacation. There are often beside the lake (e.g. Birchcliff, Jarvis Bay) or near natural park (e.g. Banff, Jasper). The abnormal increase of the dwelling value may be related to its good location and nature environment.\n2.2 Monthly cost # Median Monthly shelter cost mon.cost.21 \u0026lt;- get_census(dataset = \u0026#34;CA21\u0026#34;, regions = regions.list, vectors = c(\u0026#34;median.cost.rent\u0026#34;=\u0026#34;v_CA21_4317\u0026#34;, \u0026#34;median.cost.own\u0026#34;=\u0026#34;v_CA21_4309\u0026#34;), level = \u0026#34;CSD\u0026#34;, geo_format = \u0026#39;sf\u0026#39;, labels = \u0026#39;short\u0026#39;) mon.cost.21 \u0026lt;- mon.cost.21 %\u0026gt;% pivot_longer(cols = starts_with(\u0026#34;median.cost\u0026#34;), names_to = \u0026#34;status\u0026#34;, names_prefix = \u0026#34;median.cost.\u0026#34;, values_to = \u0026#34;month.cost\u0026#34;) mon.cost.21$status \u0026lt;- factor(mon.cost.21$status, levels = c(\u0026#34;own\u0026#34;,\u0026#34;rent\u0026#34;), labels = c(\u0026#34;Owner Households\u0026#34;, \u0026#34;Renter Households\u0026#34;)) ggplot(mon.cost.21) + geom_sf(aes(fill=month.cost)) + theme_minimal() + theme(panel.grid = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), strip.text.x = element_text(size=12)) + coord_sf(datum=NA) + scale_fill_viridis_c(\u0026#34;Median Monthly cost\u0026#34;, labels = scales::dollar) + facet_wrap(~status) The next question I had about the housing is whether the cost of housing people spend monthly share the same distribution as the dwelling price?\nThe above figure showed the median monthly cost people spend per household. The monthly cost, in general, share the same characteristic as the dwelling price: where the dwelling value is low, the monthly cost people spend on housing is also tend to be low.\nThere are 2 interesting differences: first the cycle of higher price around Calgary is disappeared. So people who live in high value dwellings actually don’t spend significantly more monthly. It might because those high value dwellings are owned, not rented, and the cost for owner maintaining is not significantly higher. We can also notice this in the right sub-figure in figure-3, where area which has high value of dwellings actually have average or even lower monthly cost for renter households.\nThe second interesting difference is between the owner household and renter household. The monthly cost distribution tend to be the same, however, in the northeastern area, the owner households have a abnormal high cost monthly. By Google-search, the Northeastern area is agricultural area, where the owner might spend more to maintain their farm house, which not only serve as living.\n2.3 Difference between large cities and elsewhere # Number of Household of Renter and Owner household \u0026lt;- get_census(dataset = \u0026#34;CA21\u0026#34;, regions = regions.list, vectors = c(\u0026#34;status.Renter\u0026#34;=\u0026#34;v_CA21_4239\u0026#34;, \u0026#34;status.Owner\u0026#34;=\u0026#34;v_CA21_4238\u0026#34;, \u0026#34;total.house\u0026#34;=\u0026#34;v_CA21_4237\u0026#34;), level = \u0026#34;CSD\u0026#34;, labels = \u0026#39;short\u0026#39;) household \u0026lt;- household %\u0026gt;% slice_max(total.house, n=5) ggplot(household, aes(x=total.house, y=fct_reorder(`Region Name`, total.house))) + geom_col(fill=\u0026#34;cornflowerblue\u0026#34;) + theme_minimal() + theme(axis.title.y = element_blank()) + xlab(\u0026#34;Number of Households\u0026#34;) + ggtitle(\u0026#34;Districts with Top 5 Number of Household in Alberta\u0026#34;) While I looking at the household distribution, I realized the households are distributed very unevenly within Alberta. The above figure showed the districts with top 5 number of household in Alberta. As shown in the figure, the number of household in Calgary and Edmonton is extremely larger than the rest of the province. Calgary has around 500,000 households, with about 400,000 in Edmonton, while the third district in rank has less than 50,000, which means less the 10% of the number of households in Alberta. This mean the majority of households are accumulated in merely 2 districts in the province.\nThis then led to the last question: whether the housing situation has large difference between the 2 districts (Calgary and Edmonton) and elsewhere in Alberta?\n#suitable housing data vec_suit\u0026lt;- list_census_vectors(\u0026#34;CA21\u0026#34;) %\u0026gt;% filter(vector == \u0026#34;v_CA21_4293\u0026#34;) %\u0026gt;% child_census_vectors(TRUE, keep_parent = TRUE) accept \u0026lt;- get_census(dataset = \u0026#34;CA21\u0026#34;, regions = regions.list, vectors = vec_suit$vector, level = \u0026#34;CSD\u0026#34;, labels = \u0026#34;short\u0026#34;) regions.main \u0026lt;- list_census_regions(dataset, use_cache = FALSE) %\u0026gt;% filter(level == \u0026#34;CSD\u0026#34;, name %in% c(\u0026#34;Calgary\u0026#34;, \u0026#34;Edmonton\u0026#34;)) %\u0026gt;% as_census_region_list ##calgary \u0026amp; edmonton vs elsewhere rate \u0026lt;- accept %\u0026gt;% mutate(urban = case_when( GeoUID %in% regions.main$CSD ~ \u0026#34;Calgary and Edmonton\u0026#34;, TRUE ~ \u0026#34;Elsewhere\u0026#34;)) ## no identified problems rest.select \u0026lt;- rate %\u0026gt;% select(contains(\u0026#34;v_CA21\u0026#34;), -v_CA21_4293) rate$rest \u0026lt;- rowSums(rest.select) rate \u0026lt;- rate %\u0026gt;% mutate(v_CA21_4293=Households-rest) ##factor order categories \u0026lt;- vec_suit %\u0026gt;% pull(\u0026#34;vector\u0026#34;) cat_list \u0026lt;- factor(categories, ordered = TRUE) cat_name \u0026lt;- vec_suit %\u0026gt;% pull(\u0026#34;label\u0026#34;) cat_name[1]=\u0026#34;No identified problems\u0026#34; ##change column names to human readable names(rate)[grepl(\u0026#34;v_\u0026#34;, names(rate))] \u0026lt;- cat_name ##pivot_longer plot_data \u0026lt;- rate %\u0026gt;% pivot_longer(cols = cat_name[1]:cat_name[8], names_to = \u0026#34;Categories\u0026#34;, values_to = \u0026#34;Count\u0026#34;) plot_data$Categories \u0026lt;- factor(plot_data$Categories, levels = cat_name, ordered = TRUE) plot_data \u0026lt;- plot_data %\u0026gt;% group_by(Categories, urban) %\u0026gt;% summarise(Count=mean(Count, na.rm = TRUE)) # Make plots wider  #knitr::opts_chunk$set(fig.width=8, fig.height=6) ggplot(plot_data, aes(x=\u0026#34;\u0026#34;, Count, group=Categories, fill=Categories)) + geom_bar(stat = \u0026#34;identity\u0026#34;, position=\u0026#34;fill\u0026#34;, width = 1) + coord_polar(\u0026#34;y\u0026#34;, start = 0) + facet_wrap(~urban)+ theme_void() + theme(legend.position = \u0026#34;bottom\u0026#34;, axis.title = element_blank(), strip.text.x = element_text(size=15), legend.text = element_text(size=10), plot.title = element_text(size=20)) + guides(fill=guide_legend(nrow = 4)) + ggtitle(\u0026#34;Housing problems: \u0026#34;) + scale_fill_brewer(palette = \u0026#34;Set2\u0026#34;) When assessing housing situation, there are 3 criteria used:\n whether people spend 30% or more income on shelter cost. If people spend more than 30% of their income on housing, it means the cost of housing is too high. whether there are more than one person per room in the dwelling. If there are more than one person per room, it means people have to live in overcrowded dwellings, and it will be considered as “not suitable” housing for living. whether the dwelling needs major repairs.  The above pie chart showed the housing problems in 2 large cities (Calgary and Edmonton) versus elsewhere in Alberta. Overall, there are more percentage of households having identified problems in Calgary and Edmonton. Specifically, there are more people spend more than 30% of their income, and more people living in “not suitable” household. It means in these 2 cities, people are suffer from high cost of housing, they either have to spend more than 30% of their income on housing, or share a room with other to reduce the cost. While in elsewhere in Alberta, the housing situation is slightly better, but there ae more households need major repairs. Which indicates that in elsewhere (other than Calgary and Alberta), there are more percentage of households not in good condition and needs repairs, but people living in couldn’t afford to.\nComments on figures All the data downloading and pre-possessing codes can be found in the first 2 chunks in .rmd file, while figures are shown in separate chunks align with the analysis paragraphs.\nThere are 3 types of figures used in the analysis:\n  The spatial distribution of a given value: I used this showing the distribution of dwelling value and monthly cost people spend on housing. This type of figure could fulfill the purpose showing how the value/cost change in space, and can lead to interesting finding, such as the high value of dwelling cycle around Calgary. When assessing how the price of housing differ in area, this type of figure could be useful. In my figures, I selected the color pallete to make the higher price more significant in bright yellow color, while lower price in deep blue color. This could highlight the area in which having higher price. I also change the scale of legend to dollar signs, which makes more sense for the analized questions.\n  The rank of districts by value: I used geom_point and geom_col to generate the 2 figures about the higher dwelling value and larger number of household. When there are a lot of observations to show, e.g. the dwelling value in districts, geom_point could make the figure more readable; While when there are just 5 observation value to show, i.e. in number of household, geom_col could really make the higher value bar stand out and attract the readers’ attention. In both figures, I reorder the data and make it rank from high to low, which increased the readability. Besides, this type of figure required reader refer to x-axis to find the exact number value, so I used theme_minimal to remain the light grid lines to help indicate the exact value in x-axis.\n  pie chart, which I used to show the percentage of housing problems in different area. ggplot actually doesn’t have a pie chart function, so I modified this from geom_bar by adjusting the function and formulating the data. I selected the color pallete to clearly distinguish the different types of problems. (I don’t know for some reasons the figure can show perfectly in Rmd but can’t show fully in knit html file…)\n  ","permalink":"http://maanqi4.github.io/posts/cancensus/","summary":"This is a course assignment demo (GG606 Scientific Data Wrangling)\n  cancensus is a R package that can assess Statistics Canada Census data for Census year 1996, 2001, 2006, 2011, 2016 and 2021. The datasets present information from the Census of Population for various levels of geography, including provinces and territories, census metropolitan areas, communities and census tracts.\n 1. Installation and retrieve the data vectors list install.packages(\u0026#34;cancensus\u0026#34;) library(cancensus) #view avaliable Census data list_census_datasets() #view regions and vectors in a given dataset list_census_regions(\u0026#34;CA21\u0026#34;) list_census_vectors(\u0026#34;CA21\u0026#34;) #retrieve the data needed #Warning: Cached regions list may be out of date.","title":"Work with Canadian Census data"},{"content":" This is a course assignment for analyze and visualize earthquake data\n 1. Read the data in and clean it for analysis, used the readr package functions for reading and parsing data. [5 marks] My answer is written here and is explains what I did and why.\n#code  library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.0  ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0  ## ✔ readr 2.1.3 ✔ forcats 0.5.2  ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() #read data data \u0026lt;- read_csv(\u0026#34;https://jjvenky.github.io/GG606AW23/database.csv\u0026#34;, col_types = cols(Date = col_date(format = \u0026#34;%m/%d/%Y\u0026#34;), Time = col_time(format = \u0026#34;%H:%M:%S\u0026#34;))) #problems(data) #data[problems(data)$row,] #as.POSIXct(problems(data)$actual, \u0026#34;%Y-%m-%dT%H:%M:%S\u0026#34;, tz=\u0026#34;UTC\u0026#34;) #check data and omit N/A if(any(is.na(data$Date))){ data \u0026lt;- filter(data, !is.na(data$Date)) } data ## # A tibble: 23,409 × 21\r## Date Time Latitude Longitude Type Depth Depth…¹ Depth…² Magni…³\r## \u0026lt;date\u0026gt; \u0026lt;time\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1965-01-02 13:44:18 19.2 146. Earthqu… 132. NA NA 6 ## 2 1965-01-04 11:29:49 1.86 127. Earthqu… 80 NA NA 5.8\r## 3 1965-01-05 18:05:58 -20.6 -174. Earthqu… 20 NA NA 6.2\r## 4 1965-01-08 18:49:43 -59.1 -23.6 Earthqu… 15 NA NA 5.8\r## 5 1965-01-09 13:32:50 11.9 126. Earthqu… 15 NA NA 5.8\r## 6 1965-01-10 13:36:32 -13.4 167. Earthqu… 35 NA NA 6.7\r## 7 1965-01-12 13:32:25 27.4 87.9 Earthqu… 20 NA NA 5.9\r## 8 1965-01-15 23:17:42 -13.3 166. Earthqu… 35 NA NA 6 ## 9 1965-01-16 11:32:37 -56.5 -27.0 Earthqu… 95 NA NA 6 ## 10 1965-01-17 10:43:17 -24.6 178. Earthqu… 565 NA NA 5.8\r## # … with 23,399 more rows, 12 more variables: `Magnitude Type` \u0026lt;chr\u0026gt;,\r## # `Magnitude Error` \u0026lt;dbl\u0026gt;, `Magnitude Seismic Stations` \u0026lt;dbl\u0026gt;,\r## # `Azimuthal Gap` \u0026lt;dbl\u0026gt;, `Horizontal Distance` \u0026lt;dbl\u0026gt;,\r## # `Horizontal Error` \u0026lt;dbl\u0026gt;, `Root Mean Square` \u0026lt;dbl\u0026gt;, ID \u0026lt;chr\u0026gt;, Source \u0026lt;chr\u0026gt;,\r## # `Location Source` \u0026lt;chr\u0026gt;, `Magnitude Source` \u0026lt;chr\u0026gt;, Status \u0026lt;chr\u0026gt;, and\r## # abbreviated variable names ¹​`Depth Error`, ²​`Depth Seismic Stations`,\r## # ³​Magnitude\r Here col_types in read_csv function was defined with format to match the format of Date and Time in the .csv file.\nNote:\nThere are 3 rows in which the Date and Time were written different format compared with the rest of the dataset. Although it’s possible to convert them to acessible format, for example using as.POSIXct(problems(data)$actual, \u0026quot;%Y-%m-%dT%H:%M:%S\u0026quot;, tz=\u0026quot;UTC\u0026quot;).The current method somehow could successfully return the correct Date, and the following question doesn’t necessary need the exact time of the earthquakes, plus I’m not sure about the timezone in the Date written in those 3 rows. So I just ignored those 3 rows as this warning would not affect following analysis.\n2. Did more earthquakes happen on weekends or weekdays? [5 marks] #code  #select Earthquake Earthquake \u0026lt;- subset(data, Type == \u0026#34;Earthquake\u0026#34;) #convert Date to day in a week Earthquake$wday \u0026lt;- weekdays(Earthquake$Date) #reorder days in week (so it\u0026#39;ll be in good order in figure\u0026#39;s legend) Earthquake$wday \u0026lt;- factor(Earthquake$wday, levels = c(\u0026#34;Monday\u0026#34;,\u0026#34;Tuesday\u0026#34;,\u0026#34;Wednesday\u0026#34;,\u0026#34;Thursday\u0026#34;, \u0026#34;Friday\u0026#34;,\u0026#34;Saturday\u0026#34;,\u0026#34;Sunday\u0026#34;)) #separate weekdays and weekend Earthquake \u0026lt;- mutate(Earthquake, wday_ID = case_when(wday == \u0026#34;Sunday\u0026#34; ~ \u0026#34;Weekends\u0026#34;, wday == \u0026#34;Saturday\u0026#34; ~ \u0026#34;Weekends\u0026#34;, TRUE ~ \u0026#34;Weekdays\u0026#34;)) #check dataset before plotting  #Earthquake #plot ggplot(data = Earthquake) + geom_bar(aes(wday_ID,fill=wday), alpha=1) + theme_classic() + scale_fill_brewer(palette = \u0026#34;Set3\u0026#34;) + labs(title = \u0026#34;Amount of Earthquake happened on Weekdays and Weekends\u0026#34;, x=\u0026#34;\u0026#34;,fill=\u0026#34;Day in week\u0026#34;) The above analysis demonstrates the amount of earthquakes happened on weekdays and weekends in the dataset. There are significantly more earthquakes happened on weekdays, however, when it comes to the number of earthquakes per day, it doesn’t show any preference between weekdays or weekends. There are 5 days in a week could be counted as weekdays, while only 2 days are weekends. If earthquakes are equally distributed in every day, then when numbers added up, it will be more earthquakes on weekdays than weekends as there are more days in weekdays.\n3. Has there been any change in the frequency of earthquakes? [5 marks] #code  #create new variable Year Earthquake \u0026lt;- mutate(Earthquake, Year = format(Date, \u0026#34;%Y\u0026#34;)) #(for setting figure axis) #count minmax of Year and number of earthquakes minyear \u0026lt;- min(Earthquake$Year) maxyear \u0026lt;- max(Earthquake$Year) counteq \u0026lt;- Earthquake %\u0026gt;% count(Earthquake$Year) maxeq \u0026lt;- max(counteq$n) maxy \u0026lt;- ceiling(maxeq/100)*100 #plot ggplot(data = Earthquake) + geom_bar(aes(Year),fill=\u0026#34;cornflowerblue\u0026#34;) + theme_minimal() + scale_fill_brewer(palette = \u0026#34;Set3\u0026#34;) + labs(title = paste0(\u0026#34;Amount of earthquake each year (\u0026#34;, minyear, \u0026#34;-\u0026#34;, maxyear, \u0026#34;)\u0026#34;)) + scale_x_discrete(breaks = seq(minyear,maxyear,by=5)) + scale_y_continuous(breaks = seq(0,maxy,by=100), expand = c(0,0)) + coord_cartesian(ylim = c(0,maxy)) The above analysis demonstrates the trend of earthquake from 1965 to 2016. The number of earthquake in each year showed a steady increase in the given time period. Therefore, we concluded that the frequency of earthquake in each year has a steady increase between the year of 1965 and 2016.\n4. Where were there more earthquakes in the 1980s, South America or North America? [5 marks] #code  library(maps) ## ## Attaching package: 'maps'\r## The following object is masked from 'package:purrr':\r## ## map\r #select earthquakes in 1980s eq.1980s \u0026lt;- filter(Earthquake, Year \u0026gt;= 1980 \u0026amp; Year \u0026lt;= 1989) #eq.1980s #First, plot a scatterplot map with all the earthquakes in 1980s world_map \u0026lt;- map_data(\u0026#34;world\u0026#34;) ggplot(NULL) + geom_polygon(data = world_map, aes(x=long,y=lat,group=group), fill=\u0026#34;azure3\u0026#34;) + geom_point(data = eq.1980s, aes(x=Longitude, y=Latitude), color = \u0026#34;cornflowerblue\u0026#34;, size=.5) + theme_void() + ggtitle(\u0026#34;Earthquakes in 1980s\u0026#34;) ## Notice a lot of earthquakes located in ocean, some on land ##define South/North America ##only count earthquakes happened on land ##convert (lat,lon) to continent (point in polygon question) ##https://stackoverflow.com/questions/21708488/get-country-and-continent-from-longitude-and-latitude-point-in-r library(sp) library(rworldmap) ## ### Welcome to rworldmap ###\r## For a short introduction type : vignette('rworldmap')\r latlon2Con \u0026lt;- function(lat,lon){ sPDF \u0026lt;- getMap() points = data.frame(lon=lon,lat=lat) pointsSP = SpatialPoints(points, proj4string = CRS(proj4string(sPDF))) indices = over(pointsSP, sPDF) return(indices$REGION) #return(indice$ADMIN) } eq.1980s.land \u0026lt;- mutate(eq.1980s, Continent = latlon2Con(eq.1980s$Latitude,eq.1980s$Longitude)) eq.1980s.land \u0026lt;- filter(eq.1980s.land, !is.na(Continent)) ggplot(data = eq.1980s.land) +geom_bar(aes(Continent), fill=\u0026#34;cornflowerblue\u0026#34;)+ theme_classic() + ggtitle(\u0026#34;Earthquakes in 1980s (on land)\u0026#34;) ##2. consider earthquakes both on land and ocean ##if earthquake located in ocean, find the nearest continent ##calculate centriods of each countries #library(sf) #world_map_sf \u0026lt;- st_as_sf(world_map, coords = c(\u0026#34;long\u0026#34;,\u0026#34;lat\u0026#34;), crs=4326) #crs = 4326 means lat lon are interpreted as WGS 84 coordinates #world_cens \u0026lt;- st_centroid(world_map_sf) #create index to speed up the nearest function #grid_index \u0026lt;- st_make_grid(world_cens) #find nearest centroids  #point \u0026lt;- data.frame(lon=100,lat=23) #point_sf \u0026lt;- st_as_sf(point, coords = c(\u0026#34;lon\u0026#34;,\u0026#34;lat\u0026#34;), crs=4326,agr=\u0026#34;contant\u0026#34;) #nearest_points \u0026lt;- st_nearest_points(world_cens,point_sf,index=grid_index) #still takes too much time and memory, ignore this method! #nearest_points The above analysis demonstrates the geospatial distribution of earthquakes in 1980s and the number of earthquake in each continent.\nFirst, I demonstrated the raw data of earthquakes in 1980s and their distribution. The earthquakes were mostly taken place in the major earthquake zone, while some of the earthquakes happened on land, a large number of them happened in the ocean area. Here, when identifying the origin location(continent) of earthquakes, I excluded those in the ocean.\nIdealy, there should be 2 algorithms identifying the origin location of the earthquakes: point-in-polygon and finding-nearest-points. After experiments, I realized it’s not feasible to calculate nearest points using such large dataset on my laptop. Here I modified the function in rworldmap package and in this solution. The latlon2Con function in the code could decide whether the location (indexed by latitude and longitude) falls into a certain country’s polygon, and return the continent where that country belongs. However, for earthquakes happened in ocean, it will return N/A value.\nWith the above function, we can assigned a Continent value for each earthquakes on land in 1980s, and plot and bar plot indicating the number of earthquakes on each continent in 1980s. Here thh bar plot shows that there are more earthquakes in South America than North America in 1980s.\n5. Has there been any geographic shifts in the distribution of earthquakes? [10 marks] #code  eq.land \u0026lt;- mutate(Earthquake, Continent = latlon2Con(Earthquake$Latitude,Earthquake$Longitude)) eq.land \u0026lt;- filter(eq.land, !is.na(Continent)) eq.land.num \u0026lt;- eq.land %\u0026gt;% count(Year, Continent) %\u0026gt;% group_by(Continent) ggplot(eq.land.num, aes(x=Year, y=Continent, fill=n)) + geom_tile() + theme_classic() + labs(title = paste0(\u0026#34;Earthquakes on land (\u0026#34;, minyear, \u0026#34;-\u0026#34;, maxyear, \u0026#34;)\u0026#34;),fill=\u0026#34;Num\u0026#34;) + scale_x_discrete(breaks = seq(minyear,maxyear,by=5)) + scale_y_discrete(expand = c(0,0)) + scale_fill_distiller(palette = \u0026#34;YlOrRd\u0026#34;, direction = 1) The above analysis demonstrates the number of earthquakes in each continent from the year of 1965 to 2016. Similarly, here I only considered earthquakes happened on land, using the same function as in the previous question. According to the figure, there is no major shifts of the location(origin continent) of earthquakes in the given time period. Asia and South America constantly have more earthquakes compared to other continents.\n6. Comment on how lessons from Wilke’s Fundementals of Data Visualization were applied to each figure with specific reference to book sections [5 marks] Overall, I tried to produce clean and readable figure, with clear coordinates and readable titles, legends and axis.\n  For the figure illustrating the number of earthquakes on weekdays and weekends: It is a figure to visualize amounts, I chose stacked bar plots. In this way, the figure could display the amount of earthquakes in weekdays and weekend while comparing earthquakes happened in each day, showing more information at the same time. I also assigned an order to the weekdays column so that it will displayed in a more readable way (i.e. following the order of day in a week). Reference: Section 6.2\n  For the third question, I displayed the amount of earthquakes each year to visualize how the frequency of earthquakes (in each year) changed in the given time period. Similar to the second question, here I also visualized the amount, but with more data points. Another potential way is visualizing the trends with smoothed line plot, however, I want readers have a clearer sense of specific numbers without omitting too much information, so I still used the bar plot. This time, in order to give a clearer reference of numbers, I changed to theme_minimal() to display hidden grid lines on the figure. This design will allow readers locate the values of each bar easier while still maintain a clear visualization style. Reference: Section 25, Section 28.3\n  For the 4th question, I used 2 figures:\n    Global distribution of earthquakes in 1980s.\n  Although it is not directly asked, for question involved with further analysis, I prefer to offer the raw data first. This will give reader (and myself) a more general idea of the data, and avoid future misunderstanding or mistakes when further analysis the data. Here, I displayed the raw distribution of earthquakes globally, showing some general knowledge of where the earthquakes usually take place, and leads to further discussion that lots of earthquakes happened in the ocean area, while I only consider earthquakes on land in the following analysis. This will also avoid future questions from readers e.g. why the number of earthquakes counted in the next figure is much smaller compared to the numbers shown in the 2nd and 3rd question.\n  Here I used theme_void() to only display the global landmass and earthquakes location. The color of landmass and earthquakes were carefully chose so that it can clearly distinguish earthquakes from landmass. It might be better to use Robinson Projections so that the high latitude area is less distorted, but I have problems finding this projection in coord_map(), besides we are not focused on high latitude in this question, so I used the default settings. Reference: Section 4.1\n    Number of earthquakes on each continents. Here it’s also required to display “amounts”, so I used the simple bar plots to make the figure clear.\n  For the 5th question, I used a heatmap to illustrate how the number of earthquakes changes on each continent in each year. If there was geographic shift, say, from continent to continent, then it would be easily spotted on the heatmap figure. Besides, I also selected color palette to make the fill colors represent the number values in a more custom way. Reference: Section 6.3, Section 4.2  ","permalink":"http://maanqi4.github.io/posts/earthquakes/","summary":"This is a course assignment for analyze and visualize earthquake data\n 1. Read the data in and clean it for analysis, used the readr package functions for reading and parsing data. [5 marks] My answer is written here and is explains what I did and why.\n#code  library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.0  ## ✔ tibble 3.","title":"Earthquakes data analysis demo"},{"content":"1. Main Settings Experiment name: f.e12.FAMIPCN.f19_f19\n f f compset;\n“F” compsets use CAM,CLM, CICE(prescribed-thermo), DOCN(prescribed-SST). e12 cesm1.2.2 (model version) FAMIPCN AMIP run for CMIP5 protocol with CLM/CN:\nAMIP_CAM4_CLM40%CN_CICE%PRES_DOCN%DOM_RTM_SGLC_SWAV\nAMIP: time, AMIP runs\nCAM4: atmosphere model CAM4\nCLM40: land model, clm4.0\nCN (carbon-nitrogen) model version: a biogeochemistry model that simulates the carbon and nitrogen cycles\nCICE%PRES: prescribed sea ice\nDOCN%DOM: DOCN data ocean mode\nRTM: river transport model, land river runoff\nSGLC: Land Ice, Stub glacier (land ice) component\nSWAV: Wave, Stub wave component f19_f19 1.9x2.5_1.9x2.5\natm_grid,lnd_grid,ice_grid,ocn_grid: 1.9x2.5\natm_grid_type,ocn_grid_type: finite volume   1.1 output frequency 1.2 enable -COSP option In user_nl_cam\n\u0026amp;cospsimulator_nl docosp = .true. cosp_amwg = .true. cosp_cfmip_da = .true. #daily simulator  2. Long Simulations 3. Short Simulations 3.1 Initial Conditions 3.1.1 generate initial conditions Base: f.e12.FAMIPCN.f19_f19.short.Parent Modify env_run.xml\n./xmlchange RUN_STARTDATE = \u0026#39;1978-10-01\u0026#39; ./xmlchange STOP_OPTIONS = \u0026#39;nmonths\u0026#39; ./xmlchange STOP_N = \u0026#39;3\u0026#39; ./xmlchange RESUBMIT = \u0026#39;3\u0026#39; Modify user_nl_cam\ninithist = \u0026#39;MONTHLY\u0026#39; inithist_all = .true. Also remember to set relevant perturbed parameters in user_nl_cam\ncldfrc_rhminl = 0.91 cldopt_rliqocean = 14 hkconv_cmftau = 1800 cldfrc_rhminh = 0.80 zmconv_tau = 3600 After 4 runs consecutively, in $ARCHIVE/rest, there will be 4 directories, containing initial files for 1979-01-01, 1979-04-01, 1979-07-01, 1979-10-01. Example file list in 1979-01-01:\nf.e12.FAMIPCN.f19_f19.short.Parent.cam.h0.1978-12.nc f.e12.FAMIPCN.f19_f19.short.Parent.cam.h1.1978-10-01-00000.nc f.e12.FAMIPCN.f19_f19.short.Parent.cam.h2.1978-12-30-10800.nc f.e12.FAMIPCN.f19_f19.short.Parent.cam.i.1979-01-01-00000.nc f.e12.FAMIPCN.f19_f19.short.Parent.cam.r.1979-01-01-00000.nc f.e12.FAMIPCN.f19_f19.short.Parent.cam.rh0.1979-10-31-00000.nc f.e12.FAMIPCN.f19_f19.short.Parent.cam.rs.1979-01-01-00000.nc f.e12.FAMIPCN.f19_f19.short.Parent.cice.r.1979-01-01-00000.nc f.e12.FAMIPCN.f19_f19.short.Parent.clm2.h0.1978-12.nc f.e12.FAMIPCN.f19_f19.short.Parent.clm2.r.1979-01-01-00000.nc f.e12.FAMIPCN.f19_f19.short.Parent.clm2.rh0.1979-01-01-00000.nc f.e12.FAMIPCN.f19_f19.short.Parent.cpl.r.1979-01-01-00000.nc f.e12.FAMIPCN.f19_f19.short.Parent.docn.rs1.1979-01-01-00000.bin f.e12.FAMIPCN.f19_f19.short.Parent.rtm.h0.1978-12.nc f.e12.FAMIPCN.f19_f19.short.Parent.rtm.r.1979-01-01-00000.nc f.e12.FAMIPCN.f19_f19.short.Parent.rtm.rh0.1979-01-01-00000.nc rpointer.atm rpointer.drv rpointer.ice rpointer.lnd rpointer.ocn rpointer.rof scripts: csh.init.diff.niagara generate initial conditions for each parameter setting (modify and submit 50cases: f.e12.FAMIPCN.f19_f19.init.diff.*)\n3.2 start initial runs (same initial condition for each parameter setting) scripts:\n bash ==init.short.parent.niagara== to generate parent run files [change runstartdate] sbatch ==csh.init.short.child.niagara== to run 50 ensembles in given start date [change runstartdate] f.e12.FAMIPCN.f19_f19.short.op.$i.$runstartdate munual run 4 parent runs in f.e12.FAMIPCN.f19_f19.init.op.$runstartdate genarated by ==init.short.parent.niagara== [munually modify parameters in user_nl_cam]  Modify initial files, in user_nl_cam (use the initial files generate by $3.1$)\nncdata = \u0026#39;f.e12.FAMIPCN.f19_f19.short.Parent.cam.i.1979-01-01-00000.nc\u0026#39; #initial files for CAM By default, this file should soft link to $RUN, setting RUN_REFCASE and GET_REFCASE=TRUE to make true of this. One can also indicate specific path for initial file, but if initial conditions files are not in $RUN directory, make sure to also set other model\u0026rsquo;s .r. files to avoid error. I prefer to ponit all initial files to $RUN for an easier operation. Modify env_run.xml\n./xmlchange RUN_TYPE=hybrid #default is hybrid for FAMIPCN, but make sure set it up anyway ./xmlchange RUN_STARTDATE=1979-01-01 ./xmlchange RUN_REFCASE=f.e12.FAMIPCN.f19_f19.short.Parent #which REFCASE to look for in /inputdata/ccsm_init ./xmlchange RUN_REFDATE=1979-01-01 #which date/directory in REFCASE to use ./xmlchange GET_REFCASE=TRUE #soft link initial files to $RUN directory One can have a case start from 1979-01-01 but using initial conditions in 1979-04-01 by setting RUN_STARTDATE=1979-01-01 and RUN_REFDATE=1979-04-01. (If these is a REFCASE started in 1979-04-01 existed.) Build a new case or rebuild the original case\n3.3 initial runs with different initial conditions for different parameter setting scripts: csh.init.diff.short.child.parent\n","permalink":"http://maanqi4.github.io/posts/short-long-experiment-outline/","summary":"1. Main Settings Experiment name: f.e12.FAMIPCN.f19_f19\n f f compset;\n“F” compsets use CAM,CLM, CICE(prescribed-thermo), DOCN(prescribed-SST). e12 cesm1.2.2 (model version) FAMIPCN AMIP run for CMIP5 protocol with CLM/CN:\nAMIP_CAM4_CLM40%CN_CICE%PRES_DOCN%DOM_RTM_SGLC_SWAV\nAMIP: time, AMIP runs\nCAM4: atmosphere model CAM4\nCLM40: land model, clm4.0\nCN (carbon-nitrogen) model version: a biogeochemistry model that simulates the carbon and nitrogen cycles\nCICE%PRES: prescribed sea ice\nDOCN%DOM: DOCN data ocean mode\nRTM: river transport model, land river runoff","title":"Generate Short/Long Perturbed Parameter Ensembles (PPE) with CESM"}]